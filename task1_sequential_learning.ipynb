{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries and modules\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"results (1)\" # Directory containing extracted features\n",
    "\n",
    "# Function to load the extracted features stored in .pkl files\n",
    "def load_all_pickles(directory):\n",
    "    pickle_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pkl'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                pickle_data[filename] = pickle.load(f)\n",
    "    return pickle_data\n",
    "\n",
    "# Loading all .pkl files into a dictionary\n",
    "data_dict = load_all_pickles(directory_path)\n",
    "\n",
    "# Extracting training and validation data('held out datasets')\n",
    "train_task1_data = data_dict['train_task1.pkl']\n",
    "eval_task1_data = data_dict['eval_task1.pkl']\n",
    "\n",
    "# Converting the lists to numpy arrays\n",
    "train_task1_arr = np.array(train_task1_data)\n",
    "eval_task1_arr = np.array(eval_task1_data)\n",
    "train_task1_arr=np.reshape(train_task1_arr,(10,2500,1024))\n",
    "eval_task1_arr=np.reshape(eval_task1_arr,(10,2500,1024))\n",
    "\n",
    "# Loading the labels for D_1\n",
    "data = torch.load(r\"dataset\\part_one_dataset\\train_data\\1_train_data.tar.pth\")\n",
    "labels_list = data['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypes calculated for classes: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Prototype shape: torch.Size([10, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to group features by their class labels\n",
    "class_features = defaultdict(list)\n",
    "\n",
    "# Group feature embeddings by their labels from the training data\n",
    "for feature, label in zip(train_task1_arr[0], labels_list):\n",
    "    # Append each feature to the corresponding label's list\n",
    "    class_features[label.item()].append(feature)\n",
    "\n",
    "# Calculate the prototype (mean feature vector) for each class\n",
    "prototypes = {\n",
    "    label: torch.mean(\n",
    "        torch.stack([\n",
    "            # Convert features to tensors if they are NumPy arrays\n",
    "            torch.tensor(feature) if isinstance(feature, np.ndarray) else feature \n",
    "            for feature in features\n",
    "        ]),\n",
    "        dim=0  # Take the mean along the feature axis\n",
    "    )\n",
    "    for label, features in class_features.items()  # Iterate over each class and its features\n",
    "}\n",
    "\n",
    "# Stack the prototypes into a tensor for efficient computations\n",
    "prototype_tensors = torch.stack([prototypes[label] for label in sorted(prototypes.keys())])\n",
    "\n",
    "# Create a tensor of class labels corresponding to the prototypes\n",
    "prototype_labels = torch.tensor(sorted(prototypes.keys()))\n",
    "\n",
    "# Display the calculated prototypes and their shape\n",
    "print(\"Prototypes calculated for classes:\", prototype_labels)\n",
    "print(\"Prototype shape:\", prototype_tensors.shape)  # Shape: [num_classes, feature_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on training data: 98.72%\n"
     ]
    }
   ],
   "source": [
    "def lwp_classifier(features, prototypes, prototype_labels):\n",
    "    \"\"\"\n",
    "    Classifies input features based on the nearest prototype using the Learning with Prototypes (LWP) approach.\n",
    "\n",
    "    Args:\n",
    "    - features (torch.Tensor or np.ndarray): Input feature vectors to classify. Shape: [num_samples, feature_dim].\n",
    "    - prototypes (torch.Tensor or np.ndarray): Prototype vectors for each class. Shape: [num_classes, feature_dim].\n",
    "    - prototype_labels (torch.Tensor): Class labels corresponding to the prototypes. Shape: [num_classes].\n",
    "\n",
    "    Returns:\n",
    "    - predictions (torch.Tensor): Predicted class labels for each input feature. Shape: [num_samples].\n",
    "    \"\"\"\n",
    "    # Ensure inputs are PyTorch tensors\n",
    "    if isinstance(features, np.ndarray):\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "    if isinstance(prototypes, np.ndarray):\n",
    "        prototypes = torch.tensor(prototypes, dtype=torch.float32)\n",
    "\n",
    "    # Compute pairwise distances between features and prototypes\n",
    "    # distances[i, j] = distance between i-th sample and j-th prototype\n",
    "    distances = torch.cdist(features, prototypes)\n",
    "\n",
    "    # Find the index of the closest prototype for each feature\n",
    "    # predictions[i] = index of the closest prototype for the i-th feature\n",
    "    predictions = torch.argmin(distances, dim=1)\n",
    "\n",
    "    # Map prototype indices to their corresponding class labels\n",
    "    return prototype_labels[predictions]\n",
    "\n",
    "# Classify training data using the LWP classifier\n",
    "# `train_task1_arr[0]` contains the feature embeddings for the first task\n",
    "pred = lwp_classifier(train_task1_arr[0], prototype_tensors, prototype_labels)\n",
    "\n",
    "# Ensure the predictions are stored as a PyTorch tensor\n",
    "if not isinstance(pred, torch.Tensor):\n",
    "    pred = torch.tensor(pred, dtype=torch.int64)\n",
    "\n",
    "# Convert ground-truth labels to a PyTorch tensor\n",
    "labels_list = torch.tensor(labels_list, dtype=torch.int64)\n",
    "\n",
    "# Compute classification accuracy\n",
    "# (pred == labels_list): Boolean tensor indicating correct predictions\n",
    "# .float(): Convert to float for mean computation\n",
    "# .mean(): Fraction of correct predictions\n",
    "accuracy = (pred == labels_list).float().mean().item() * 100\n",
    "\n",
    "# Print the classification accuracy on the training data\n",
    "print(f\"Classification accuracy on training data: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prototypes(prototypes, prototype_labels, new_features, pseudo_labels, alpha):\n",
    "    \"\"\"\n",
    "    Updates the class prototypes by incorporating new features using a weighted average.\n",
    "\n",
    "    Args:\n",
    "    - prototypes (torch.Tensor): Current prototype vectors for each class. Shape: [num_classes, feature_dim].\n",
    "    - prototype_labels (torch.Tensor): Class labels corresponding to the prototypes. Shape: [num_classes].\n",
    "    - new_features (torch.Tensor): Feature embeddings of the new data. Shape: [num_samples, feature_dim].\n",
    "    - pseudo_labels (torch.Tensor): Pseudo labels for the new data points. Shape: [num_samples].\n",
    "    - alpha (float): Weighting factor to control the influence of the old prototypes.\n",
    "\n",
    "    Returns:\n",
    "    - updated_prototypes (torch.Tensor): Updated prototype vectors. Shape: [num_classes, feature_dim].\n",
    "    \"\"\"\n",
    "    # Clone the prototypes to avoid modifying the original tensor\n",
    "    updated_prototypes = prototypes.clone()\n",
    "\n",
    "    # Loop over unique class labels\n",
    "    unique_labels = torch.unique(prototype_labels)\n",
    "    \n",
    "    alpha=alpha*2500\n",
    "    for label in unique_labels:\n",
    "        # Create a boolean mask for the current label\n",
    "        mask = (pseudo_labels == label)\n",
    "\n",
    "        # Extract the features corresponding to the current class label\n",
    "        new_class_features = new_features[mask]\n",
    "\n",
    "        # Ensure `new_class_features` is a PyTorch tensor\n",
    "        if not isinstance(new_class_features, torch.Tensor):\n",
    "            new_class_features = torch.tensor(new_class_features, dtype=torch.float32)\n",
    "\n",
    "        # Handle edge case: If no new features exist for the label, skip updating\n",
    "        if new_class_features.size(0) == 0:\n",
    "            continue\n",
    "\n",
    "        # Compute the updated prototype using a weighted average:\n",
    "        updated_prototypes[label] = (\n",
    "            alpha * prototypes[label] + new_class_features.sum(dim=0)\n",
    "        ) / (alpha + mask.sum())\n",
    "\n",
    "    return updated_prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prototypes(prototypes, prototype_labels, data, labels):\n",
    "    \"\"\"\n",
    "    Evaluates the classification accuracy of prototypes on a given dataset.\n",
    "\n",
    "    Args:\n",
    "    - prototypes (torch.Tensor): Prototype vectors for each class. Shape: [num_classes, feature_dim].\n",
    "    - prototype_labels (torch.Tensor): Class labels corresponding to the prototypes. Shape: [num_classes].\n",
    "    - data (torch.Tensor or np.ndarray): Feature embeddings to classify. Shape: [num_samples, feature_dim].\n",
    "    - labels (torch.Tensor or np.ndarray): True labels for the data. Shape: [num_samples].\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): Classification accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    # Use the LWP classifier to predict labels for the input data\n",
    "    predictions = lwp_classifier(data, prototypes, prototype_labels)\n",
    "\n",
    "    # Ensure both predictions and labels are PyTorch tensors\n",
    "    if isinstance(predictions, np.ndarray):\n",
    "        predictions = torch.tensor(predictions, dtype=torch.int64)\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    correct = (predictions == labels).sum().item()  # Count where predictions match the labels\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total = labels.size(0)\n",
    "\n",
    "    # Compute accuracy as a percentage\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset D_1...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_1 on all previous held-out datasets...\n",
      "Accuracy of f_1 on D_1: 98.32%\n",
      "Processing dataset D_2...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_2 on all previous held-out datasets...\n",
      "Accuracy of f_2 on D_1: 98.36%\n",
      "Accuracy of f_2 on D_2: 97.84%\n",
      "Processing dataset D_3...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_3 on all previous held-out datasets...\n",
      "Accuracy of f_3 on D_1: 98.16%\n",
      "Accuracy of f_3 on D_2: 97.76%\n",
      "Accuracy of f_3 on D_3: 98.16%\n",
      "Processing dataset D_4...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_4 on all previous held-out datasets...\n",
      "Accuracy of f_4 on D_1: 98.16%\n",
      "Accuracy of f_4 on D_2: 97.76%\n",
      "Accuracy of f_4 on D_3: 98.04%\n",
      "Accuracy of f_4 on D_4: 97.92%\n",
      "Processing dataset D_5...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_5 on all previous held-out datasets...\n",
      "Accuracy of f_5 on D_1: 98.20%\n",
      "Accuracy of f_5 on D_2: 97.68%\n",
      "Accuracy of f_5 on D_3: 97.96%\n",
      "Accuracy of f_5 on D_4: 98.00%\n",
      "Accuracy of f_5 on D_5: 97.92%\n",
      "Processing dataset D_6...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_6 on all previous held-out datasets...\n",
      "Accuracy of f_6 on D_1: 98.12%\n",
      "Accuracy of f_6 on D_2: 97.84%\n",
      "Accuracy of f_6 on D_3: 98.00%\n",
      "Accuracy of f_6 on D_4: 97.92%\n",
      "Accuracy of f_6 on D_5: 97.96%\n",
      "Accuracy of f_6 on D_6: 98.40%\n",
      "Processing dataset D_7...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_7 on all previous held-out datasets...\n",
      "Accuracy of f_7 on D_1: 98.16%\n",
      "Accuracy of f_7 on D_2: 97.76%\n",
      "Accuracy of f_7 on D_3: 97.92%\n",
      "Accuracy of f_7 on D_4: 97.92%\n",
      "Accuracy of f_7 on D_5: 98.00%\n",
      "Accuracy of f_7 on D_6: 98.36%\n",
      "Accuracy of f_7 on D_7: 97.40%\n",
      "Processing dataset D_8...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_8 on all previous held-out datasets...\n",
      "Accuracy of f_8 on D_1: 98.00%\n",
      "Accuracy of f_8 on D_2: 97.76%\n",
      "Accuracy of f_8 on D_3: 97.80%\n",
      "Accuracy of f_8 on D_4: 97.84%\n",
      "Accuracy of f_8 on D_5: 97.88%\n",
      "Accuracy of f_8 on D_6: 98.36%\n",
      "Accuracy of f_8 on D_7: 97.36%\n",
      "Accuracy of f_8 on D_8: 97.56%\n",
      "Processing dataset D_9...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_9 on all previous held-out datasets...\n",
      "Accuracy of f_9 on D_1: 98.08%\n",
      "Accuracy of f_9 on D_2: 97.72%\n",
      "Accuracy of f_9 on D_3: 97.84%\n",
      "Accuracy of f_9 on D_4: 97.96%\n",
      "Accuracy of f_9 on D_5: 97.84%\n",
      "Accuracy of f_9 on D_6: 98.28%\n",
      "Accuracy of f_9 on D_7: 97.36%\n",
      "Accuracy of f_9 on D_8: 97.60%\n",
      "Accuracy of f_9 on D_9: 97.68%\n",
      "Processing dataset D_10...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_10 on all previous held-out datasets...\n",
      "Accuracy of f_10 on D_1: 98.04%\n",
      "Accuracy of f_10 on D_2: 97.76%\n",
      "Accuracy of f_10 on D_3: 97.88%\n",
      "Accuracy of f_10 on D_4: 97.96%\n",
      "Accuracy of f_10 on D_5: 97.84%\n",
      "Accuracy of f_10 on D_6: 98.28%\n",
      "Accuracy of f_10 on D_7: 97.36%\n",
      "Accuracy of f_10 on D_8: 97.60%\n",
      "Accuracy of f_10 on D_9: 97.64%\n",
      "Accuracy of f_10 on D_10: 97.88%\n"
     ]
    }
   ],
   "source": [
    "# Iteratively update prototypes and evaluate on held-out datasets\n",
    "for i in range(1, 11):  # Process datasets D_2 to D_10\n",
    "    print(f\"Processing dataset D_{i}...\")\n",
    "    \n",
    "    # Load training data for the current dataset\n",
    "    train_data = train_task1_arr[i - 1]  # Features for the current dataset\n",
    "    \n",
    "    # Generate pseudo-labels for the training data using the current prototypes\n",
    "    pseudo_labels = lwp_classifier(train_data, prototype_tensors, prototype_labels)\n",
    "    \n",
    "    # Update prototypes using the new training data and pseudo-labels\n",
    "    prototype_tensors = update_prototypes(prototype_tensors, prototype_labels, train_data, pseudo_labels, alpha=0.2)\n",
    "    print(f\"Prototypes updated!\\nEvaluating updated model f_{i} on all previous held-out datasets...\")\n",
    "    \n",
    "    # Evaluate the updated prototypes on all previous datasets\n",
    "    for j in range(1, i + 1):  # Evaluate on D_1 to D_i\n",
    "        # Load evaluation data for dataset D_j\n",
    "        val_data = torch.load(f\"dataset/part_one_dataset/eval_data/{j}_eval_data.tar.pth\")\n",
    "        val_labels = val_data['targets']  # Ground-truth labels for the validation data\n",
    "        val_feat = eval_task1_arr[j - 1]  # Feature embeddings for dataset D_j\n",
    "        \n",
    "        # Compute accuracy of the updated model f_i on dataset D_j\n",
    "        accuracy = evaluate_prototypes(prototype_tensors, prototype_labels, val_feat, val_labels)\n",
    "        print(f\"Accuracy of f_{i} on D_{j}: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the final prototype tensors after processing all datasets\n",
    "with open('f_10_ptensors.pkl', 'wb') as f:\n",
    "    pickle.dump(prototype_tensors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
