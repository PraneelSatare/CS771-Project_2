{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"results (1)\" # Directory containing extracted features\n",
    "\n",
    "# Function to load all .pkl files from a directory\n",
    "def load_all_pickles(directory):\n",
    "    \"\"\"\n",
    "    Loads all .pkl files from the specified directory and stores them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): The path to the directory containing .pkl files.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with filenames as keys and loaded data as values.\n",
    "    \"\"\"\n",
    "    pickle_data = {}  # Initialize dictionary to store data\n",
    "\n",
    "    # Iterate through files in the specified directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pkl'):  # Only consider .pkl files\n",
    "            file_path = os.path.join(directory, filename)  # Get the full path of the file\n",
    "            with open(file_path, 'rb') as f:  # Open the file in read-binary mode\n",
    "                pickle_data[filename] = pickle.load(f)  # Load and store the data\n",
    "    \n",
    "    return pickle_data  # Return the dictionary of loaded data\n",
    "\n",
    "# Load all .pkl files from the directory\n",
    "data_dict = load_all_pickles(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the loaded pickle dictionary and convert them to numpy arrays\n",
    "\n",
    "# Load 'eval_task2.pkl' and 'train_task2.pkl' from the dictionary\n",
    "eval_task2_data = data_dict['eval_task2.pkl']\n",
    "train_task2_data = data_dict['train_task2.pkl']\n",
    "\n",
    "# Convert the 'train_task2' and 'eval_task2' data to numpy arrays\n",
    "train_task2_arr = np.array(train_task2_data)\n",
    "eval_task2_arr = np.array(eval_task2_data)\n",
    "\n",
    "# Reshape the data arrays to shape (10, 2500, 768) to match the desired structure\n",
    "train_task2_arr = np.reshape(train_task2_arr, (10, 2500, 1024))\n",
    "eval_task2_arr = np.reshape(eval_task2_arr, (10, 2500, 1024))\n",
    "\n",
    "# Load 'eval_task1.pkl' from the dictionary\n",
    "eval_task1_data = data_dict['eval_task1.pkl']\n",
    "\n",
    "# Convert 'eval_task1' data to numpy array and reshape it\n",
    "eval_task1_arr = np.array(eval_task1_data)\n",
    "eval_task1_arr = np.reshape(eval_task1_arr, (10, 2500, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the file containing the prototype tensors\n",
    "path = r\"f_10_ptensors.pkl\"\n",
    "\n",
    "# Open the file in read-binary mode and load the prototype tensors using pickle\n",
    "with open(path, 'rb') as f:\n",
    "    prototype_tensors = pickle.load(f)\n",
    "\n",
    "# Create a tensor of prototype labels (assuming 10 classes, hence labels from 0 to 9)\n",
    "prototype_labels = torch.arange(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def lwp_classifier(features, prototypes, prototype_labels):\n",
    "    \"\"\"\n",
    "    Classifies input feature vectors based on the nearest prototype using Learning with Prototypes (LWP) method.\n",
    "\n",
    "    Args:\n",
    "    - features (torch.Tensor or np.ndarray): The input feature vectors to classify. Shape: [num_samples, feature_dim].\n",
    "    - prototypes (torch.Tensor or np.ndarray): The prototype vectors for each class. Shape: [num_classes, feature_dim].\n",
    "    - prototype_labels (torch.Tensor): The class labels corresponding to the prototypes. Shape: [num_classes].\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The predicted labels for the input features. Shape: [num_samples].\n",
    "    \"\"\"\n",
    "    # Ensure inputs are PyTorch tensors, if they are NumPy arrays, convert them\n",
    "    if isinstance(features, np.ndarray):\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "    if isinstance(prototypes, np.ndarray):\n",
    "        prototypes = torch.tensor(prototypes, dtype=torch.float32)\n",
    "\n",
    "    # Compute pairwise distances between features and prototypes using Euclidean distance\n",
    "    distances = torch.cdist(features, prototypes)\n",
    "\n",
    "    # Find the index of the prototype that is closest to each feature\n",
    "    predictions = torch.argmin(distances, dim=1)\n",
    "\n",
    "    # Return the corresponding prototype labels for the predicted class\n",
    "    return prototype_labels[predictions]\n",
    "\n",
    "\n",
    "def evaluate_prototypes(prototypes, prototype_labels, data, labels):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy of the prototypes by classifying the input data and comparing predictions to true labels.\n",
    "\n",
    "    Args:\n",
    "    - prototypes (torch.Tensor): The prototype vectors for each class. Shape: [num_classes, feature_dim].\n",
    "    - prototype_labels (torch.Tensor): The class labels corresponding to the prototypes. Shape: [num_classes].\n",
    "    - data (torch.Tensor or np.ndarray): The feature vectors of the data to classify. Shape: [num_samples, feature_dim].\n",
    "    - labels (torch.Tensor or np.ndarray): The true labels of the data. Shape: [num_samples].\n",
    "\n",
    "    Returns:\n",
    "    - float: The classification accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    # Initialize counters for correct predictions and total samples\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Get predictions from the classifier\n",
    "    predictions = lwp_classifier(data, prototypes, prototype_labels)\n",
    "    \n",
    "    # Ensure predictions and labels are PyTorch tensors\n",
    "    if isinstance(predictions, np.ndarray):\n",
    "        predictions = torch.tensor(predictions, dtype=torch.int64)\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "    # Compare predictions and true labels, and sum the correct predictions\n",
    "    correct += (predictions == labels).sum().item()\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total += labels.size(0)\n",
    "\n",
    "    # Calculate accuracy as a percentage\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering_with_prototypes(target_embeddings, prototype_tensors, num_clusters=10):\n",
    "    \"\"\"\n",
    "    Perform K-Means clustering where initial cluster centers are set to the prototype tensors.\n",
    "    \n",
    "    Args:\n",
    "    - target_embeddings (torch.Tensor or np.ndarray): The feature embeddings of the target data.\n",
    "    - prototype_tensors (torch.Tensor): The current prototype tensors (size [num_classes, feature_size]).\n",
    "    - num_clusters (int): Number of clusters (10).\n",
    "    \n",
    "    Returns:\n",
    "    - centroids (torch.Tensor): The final centroids of the clusters after K-Means.\n",
    "    - cluster_labels (numpy.ndarray): The cluster labels assigned to the target embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure inputs are numpy arrays (KMeans from sklearn works with numpy)\n",
    "    if isinstance(target_embeddings, torch.Tensor):\n",
    "        target_embeddings = target_embeddings.cpu().numpy()\n",
    "    if isinstance(prototype_tensors, torch.Tensor):\n",
    "        prototype_tensors = prototype_tensors.cpu().numpy()\n",
    "    \n",
    "    # Initialize the KMeans model with the given prototypes as initial cluster centers\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init=prototype_tensors, n_init=1, random_state=42)\n",
    "    \n",
    "    # Fit KMeans to the target data\n",
    "    cluster_labels = kmeans.fit_predict(target_embeddings)\n",
    "    \n",
    "    # Get the final centroids (updated prototypes)\n",
    "    centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    \n",
    "    return centroids, cluster_labels\n",
    "\n",
    "\n",
    "def update_prototypes_with_kmeans(prototypes, prototype_labels, new_features, pseudo_labels, beta=1.0, num_clusters=10):\n",
    "    \"\"\"\n",
    "    Update the prototypes by clustering the new features using K-Means, and then averaging the existing prototypes\n",
    "    with the new cluster centroids.\n",
    "\n",
    "    Args:\n",
    "    - prototypes (torch.Tensor): Current prototypes (size: [num_classes, feature_dim]).\n",
    "    - prototype_labels (torch.Tensor): Labels corresponding to the prototypes.\n",
    "    - new_features (torch.Tensor or np.ndarray): New feature vectors to update prototypes with.\n",
    "    - pseudo_labels (torch.Tensor): Pseudo labels for the new features.\n",
    "    - beta (float): Weighting factor to balance old prototypes and new cluster centroids.\n",
    "    - num_clusters (int): Number of clusters for K-Means.\n",
    "\n",
    "    Returns:\n",
    "    - updated_prototypes (torch.Tensor): The updated prototypes after averaging with K-Means centroids.\n",
    "    \"\"\"\n",
    "    \n",
    "    updated_prototypes = prototypes.clone()  # Create a clone to update the prototypes\n",
    "\n",
    "    # Check if new_features is a tensor or a NumPy array and convert to numpy if necessary\n",
    "    if isinstance(new_features, torch.Tensor):\n",
    "        new_features = new_features.numpy()  # Convert to NumPy array if it's a tensor\n",
    "\n",
    "    # Perform K-Means clustering for the new features\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(new_features)  # Cluster new features\n",
    "\n",
    "    # Get the centroids from K-Means (new prototypes)\n",
    "    cluster_centroids = kmeans.cluster_centers_  # Shape: [num_clusters, feature_dim]\n",
    "\n",
    "    for label in prototype_labels:\n",
    "        # Create a mask for the current label\n",
    "        mask = (pseudo_labels == label)\n",
    "        new_class_features = new_features[mask]  # Features corresponding to this class label\n",
    "\n",
    "        # If no features for this label, skip updating\n",
    "        if new_class_features.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        # Perform K-Means for the new features belonging to the current class\n",
    "        # Select features for this class from clusters\n",
    "        class_features = new_class_features[cluster_labels[mask] == label]\n",
    "\n",
    "        if class_features.shape[0] == 0:\n",
    "            continue  # Skip if no features for this class\n",
    "\n",
    "        # Compute the new prototype by averaging the current prototype with the K-Means centroids for this class\n",
    "        new_prototype = (beta * prototypes[label] + class_features.mean(axis=0)) / (beta + 1)\n",
    "\n",
    "        # Update the prototype for the current label\n",
    "        updated_prototypes[label] = new_prototype\n",
    "\n",
    "    return updated_prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset D_11...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_11 on all previous held-out datasets...\n",
      "Accuracy of f_11 on D1: 98.32\n",
      "Accuracy of f_11 on D2: 97.88\n",
      "Accuracy of f_11 on D3: 98.36\n",
      "Accuracy of f_11 on D4: 98.0\n",
      "Accuracy of f_11 on D5: 98.12\n",
      "Accuracy of f_11 on D6: 98.36\n",
      "Accuracy of f_11 on D7: 97.56\n",
      "Accuracy of f_11 on D8: 97.88\n",
      "Accuracy of f_11 on D9: 97.92\n",
      "Accuracy of f_11 on D10: 98.28\n",
      "Accuracy of f_11 on D11: 90.36\n",
      "Processing dataset D_12...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_12 on all previous held-out datasets...\n",
      "Accuracy of f_12 on D1: 98.32\n",
      "Accuracy of f_12 on D2: 97.88\n",
      "Accuracy of f_12 on D3: 98.36\n",
      "Accuracy of f_12 on D4: 98.0\n",
      "Accuracy of f_12 on D5: 98.12\n",
      "Accuracy of f_12 on D6: 98.36\n",
      "Accuracy of f_12 on D7: 97.56\n",
      "Accuracy of f_12 on D8: 97.88\n",
      "Accuracy of f_12 on D9: 97.92\n",
      "Accuracy of f_12 on D10: 98.28\n",
      "Accuracy of f_12 on D11: 90.36\n",
      "Accuracy of f_12 on D12: 75.92\n",
      "Processing dataset D_13...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_13 on all previous held-out datasets...\n",
      "Accuracy of f_13 on D1: 98.32\n",
      "Accuracy of f_13 on D2: 97.88\n",
      "Accuracy of f_13 on D3: 98.36\n",
      "Accuracy of f_13 on D4: 98.0\n",
      "Accuracy of f_13 on D5: 98.12\n",
      "Accuracy of f_13 on D6: 98.36\n",
      "Accuracy of f_13 on D7: 97.56\n",
      "Accuracy of f_13 on D8: 97.88\n",
      "Accuracy of f_13 on D9: 97.92\n",
      "Accuracy of f_13 on D10: 98.28\n",
      "Accuracy of f_13 on D11: 90.36\n",
      "Accuracy of f_13 on D12: 75.92\n",
      "Accuracy of f_13 on D13: 93.56\n",
      "Processing dataset D_14...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_14 on all previous held-out datasets...\n",
      "Accuracy of f_14 on D1: 98.32\n",
      "Accuracy of f_14 on D2: 97.88\n",
      "Accuracy of f_14 on D3: 98.36\n",
      "Accuracy of f_14 on D4: 98.0\n",
      "Accuracy of f_14 on D5: 98.12\n",
      "Accuracy of f_14 on D6: 98.36\n",
      "Accuracy of f_14 on D7: 97.56\n",
      "Accuracy of f_14 on D8: 97.88\n",
      "Accuracy of f_14 on D9: 97.92\n",
      "Accuracy of f_14 on D10: 98.28\n",
      "Accuracy of f_14 on D11: 90.36\n",
      "Accuracy of f_14 on D12: 75.92\n",
      "Accuracy of f_14 on D13: 93.56\n",
      "Accuracy of f_14 on D14: 97.28\n",
      "Processing dataset D_15...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_15 on all previous held-out datasets...\n",
      "Accuracy of f_15 on D1: 98.32\n",
      "Accuracy of f_15 on D2: 97.88\n",
      "Accuracy of f_15 on D3: 98.36\n",
      "Accuracy of f_15 on D4: 98.0\n",
      "Accuracy of f_15 on D5: 98.12\n",
      "Accuracy of f_15 on D6: 98.36\n",
      "Accuracy of f_15 on D7: 97.56\n",
      "Accuracy of f_15 on D8: 97.88\n",
      "Accuracy of f_15 on D9: 97.92\n",
      "Accuracy of f_15 on D10: 98.28\n",
      "Accuracy of f_15 on D11: 90.36\n",
      "Accuracy of f_15 on D12: 75.92\n",
      "Accuracy of f_15 on D13: 93.56\n",
      "Accuracy of f_15 on D14: 97.28\n",
      "Accuracy of f_15 on D15: 97.92\n",
      "Processing dataset D_16...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_16 on all previous held-out datasets...\n",
      "Accuracy of f_16 on D1: 98.32\n",
      "Accuracy of f_16 on D2: 97.88\n",
      "Accuracy of f_16 on D3: 98.36\n",
      "Accuracy of f_16 on D4: 98.0\n",
      "Accuracy of f_16 on D5: 98.12\n",
      "Accuracy of f_16 on D6: 98.36\n",
      "Accuracy of f_16 on D7: 97.56\n",
      "Accuracy of f_16 on D8: 97.88\n",
      "Accuracy of f_16 on D9: 97.92\n",
      "Accuracy of f_16 on D10: 98.28\n",
      "Accuracy of f_16 on D11: 90.36\n",
      "Accuracy of f_16 on D12: 75.92\n",
      "Accuracy of f_16 on D13: 93.56\n",
      "Accuracy of f_16 on D14: 97.28\n",
      "Accuracy of f_16 on D15: 97.92\n",
      "Accuracy of f_16 on D16: 94.56\n",
      "Processing dataset D_17...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_17 on all previous held-out datasets...\n",
      "Accuracy of f_17 on D1: 98.32\n",
      "Accuracy of f_17 on D2: 97.88\n",
      "Accuracy of f_17 on D3: 98.36\n",
      "Accuracy of f_17 on D4: 98.0\n",
      "Accuracy of f_17 on D5: 98.12\n",
      "Accuracy of f_17 on D6: 98.36\n",
      "Accuracy of f_17 on D7: 97.56\n",
      "Accuracy of f_17 on D8: 97.88\n",
      "Accuracy of f_17 on D9: 97.92\n",
      "Accuracy of f_17 on D10: 98.28\n",
      "Accuracy of f_17 on D11: 90.36\n",
      "Accuracy of f_17 on D12: 75.92\n",
      "Accuracy of f_17 on D13: 93.56\n",
      "Accuracy of f_17 on D14: 97.28\n",
      "Accuracy of f_17 on D15: 97.92\n",
      "Accuracy of f_17 on D16: 94.56\n",
      "Accuracy of f_17 on D17: 94.56\n",
      "Processing dataset D_18...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_18 on all previous held-out datasets...\n",
      "Accuracy of f_18 on D1: 98.32\n",
      "Accuracy of f_18 on D2: 97.88\n",
      "Accuracy of f_18 on D3: 98.36\n",
      "Accuracy of f_18 on D4: 98.0\n",
      "Accuracy of f_18 on D5: 98.12\n",
      "Accuracy of f_18 on D6: 98.36\n",
      "Accuracy of f_18 on D7: 97.56\n",
      "Accuracy of f_18 on D8: 97.88\n",
      "Accuracy of f_18 on D9: 97.92\n",
      "Accuracy of f_18 on D10: 98.28\n",
      "Accuracy of f_18 on D11: 90.36\n",
      "Accuracy of f_18 on D12: 75.92\n",
      "Accuracy of f_18 on D13: 93.56\n",
      "Accuracy of f_18 on D14: 97.28\n",
      "Accuracy of f_18 on D15: 97.92\n",
      "Accuracy of f_18 on D16: 94.56\n",
      "Accuracy of f_18 on D17: 94.56\n",
      "Accuracy of f_18 on D18: 91.32\n",
      "Processing dataset D_19...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_19 on all previous held-out datasets...\n",
      "Accuracy of f_19 on D1: 98.32\n",
      "Accuracy of f_19 on D2: 97.88\n",
      "Accuracy of f_19 on D3: 98.36\n",
      "Accuracy of f_19 on D4: 98.0\n",
      "Accuracy of f_19 on D5: 98.12\n",
      "Accuracy of f_19 on D6: 98.36\n",
      "Accuracy of f_19 on D7: 97.56\n",
      "Accuracy of f_19 on D8: 97.88\n",
      "Accuracy of f_19 on D9: 97.92\n",
      "Accuracy of f_19 on D10: 98.28\n",
      "Accuracy of f_19 on D11: 90.36\n",
      "Accuracy of f_19 on D12: 75.92\n",
      "Accuracy of f_19 on D13: 93.56\n",
      "Accuracy of f_19 on D14: 97.28\n",
      "Accuracy of f_19 on D15: 97.92\n",
      "Accuracy of f_19 on D16: 94.56\n",
      "Accuracy of f_19 on D17: 94.56\n",
      "Accuracy of f_19 on D18: 91.32\n",
      "Accuracy of f_19 on D19: 76.48\n",
      "Processing dataset D_20...\n",
      "Prototypes updated!\n",
      "Evaluating updated model f_20 on all previous held-out datasets...\n",
      "Accuracy of f_20 on D1: 98.32\n",
      "Accuracy of f_20 on D2: 97.88\n",
      "Accuracy of f_20 on D3: 98.36\n",
      "Accuracy of f_20 on D4: 98.0\n",
      "Accuracy of f_20 on D5: 98.12\n",
      "Accuracy of f_20 on D6: 98.36\n",
      "Accuracy of f_20 on D7: 97.56\n",
      "Accuracy of f_20 on D8: 97.88\n",
      "Accuracy of f_20 on D9: 97.92\n",
      "Accuracy of f_20 on D10: 98.28\n",
      "Accuracy of f_20 on D11: 90.36\n",
      "Accuracy of f_20 on D12: 75.92\n",
      "Accuracy of f_20 on D13: 93.56\n",
      "Accuracy of f_20 on D14: 97.28\n",
      "Accuracy of f_20 on D15: 97.92\n",
      "Accuracy of f_20 on D16: 94.56\n",
      "Accuracy of f_20 on D17: 94.56\n",
      "Accuracy of f_20 on D18: 91.32\n",
      "Accuracy of f_20 on D19: 76.48\n",
      "Accuracy of f_20 on D20: 97.24\n"
     ]
    }
   ],
   "source": [
    "# Iterate over datasets D_11 to D_20 (i.e., the next 10 datasets)\n",
    "for i in range(10):\n",
    "    # Print the current dataset being processed (D_11 to D_20)\n",
    "    print(f\"Processing dataset D_{i + 11}...\")\n",
    "\n",
    "    # Select the training data for the current dataset (from train_task2_arr)\n",
    "    train_data = train_task2_arr[i]\n",
    "\n",
    "    # Generate pseudo-labels for the current training data using the prototypes\n",
    "    pseudo_labels = lwp_classifier(train_data, prototype_tensors, prototype_labels)\n",
    "\n",
    "    # Update the prototypes using KMeans clustering and the current training data\n",
    "    prototype_tensors = update_prototypes_with_kmeans(prototype_tensors, prototype_labels, train_data, pseudo_labels, beta=1)\n",
    "\n",
    "    # Print that prototypes have been updated\n",
    "    print(f\"Prototypes updated!\\nEvaluating updated model f_{i + 11} on all previous held-out datasets...\")\n",
    "\n",
    "    # Evaluate the updated prototypes on datasets D_1 to D_10\n",
    "    for j in range(1, 11):\n",
    "        # Load validation data for dataset D_j from eval_data\n",
    "        val_data = torch.load(f\"dataset/part_one_dataset/eval_data/{j}_eval_data.tar.pth\")\n",
    "        val_labels = val_data['targets']\n",
    "        val_feat = eval_task1_arr[j - 1]  # Features from eval_task1_arr for dataset D_j\n",
    "        \n",
    "        # Compute the accuracy of the updated model on this validation dataset\n",
    "        accuracy = evaluate_prototypes(prototype_tensors, prototype_labels, val_feat, val_labels)\n",
    "        print(f\"Accuracy of f_{i + 11} on D{j}: {accuracy}\")\n",
    "\n",
    "    # Evaluate the updated prototypes on datasets D_11 to D_(i+11)\n",
    "    for j in range(11, i + 12):\n",
    "        # Load validation data for dataset D_j from the part_two_dataset\n",
    "        val_data = torch.load(f\"dataset/part_two_dataset/eval_data/{j - 10}_eval_data.tar.pth\")\n",
    "        val_labels = val_data['targets']\n",
    "        val_feat = eval_task2_arr[j - 11]  # Features from eval_task2_arr for dataset D_j\n",
    "        \n",
    "        # Compute the accuracy of the updated model on this validation dataset\n",
    "        accuracy = evaluate_prototypes(prototype_tensors, prototype_labels, val_feat, val_labels)\n",
    "        print(f\"Accuracy of f_{i + 11} on D{j}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
